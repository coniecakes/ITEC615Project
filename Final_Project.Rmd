---
title: "Final Project STAT-615"
author: "Maxwell Miller-Golub"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
    number_sections: yes
    includes:
      after_body:
    toc: yes
    toc_float: yes
    code_download: yes
    theme: united
    df_print: kable
---
<style>
/* General text color for the document */
body {
  color: black;
}

/* Specific text size and style classes */
.footnote {
  font-size: 12px;
}

.main-text {
  font-size: 16px;
}

.header-text {
  font-size: 20px;
  font-weight: bold;
}

.section-text {
  font-size: 24px;
  font-weight: bold;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 1000)
library(tidyverse)
library(readr)
```

```{r data}
# Set current working directory to same folder that houses data file if you haven't done so already.

local_filename <- "Salary Prediction of Data Professions.csv"

Salary_Prediction_of_Data_Professions <- read_csv(local_filename)

view(Salary_Prediction_of_Data_Professions)
```

# Petro's Section

##
Offer a preliminary description of the data set. For example, indicate the size of the data source, describe the variables, and include any other data profile information that would be of interest.

This dataset provides details about data professionals, including salaries, roles, departments, and more. Useful for salary prediction, trend analysis, and HR analytics.<br>
  
  Key variables include:<br>
  
  - **FIRST NAME and LAST NAME**: Personal identifiers.<br>
  - **SEX**: Gender (Male/Female).<br>
  - **DOJ (Date of Joining)**: Employee’s start date.<br>
  - **CURRENT DATE**: Snapshot date of the data.<br>
  - **DESIGNATION**: Job role (e.g., Analyst, Manager).<br>
  - **AGE**: Employee age.<br>
  - **SALARY**: Annual salary.<br>
  - **UNIT**: Department the data professional works in.<br>
  - **LEAVES USED and LEAVES REMAINING**: Number of leaves taken and left.<br>
  - **RATINGS**: Performance ratings.<br>
  - **PAST EXP**: Previous work experience.<br>
  
```{r final_project1}
just_salary <- Salary_Prediction_of_Data_Professions %>% 
  select(SALARY) %>% 
  mutate(salary_in_thousands = SALARY/1000,
         log_salary = log(SALARY),
         sqrt_salary = sqrt(SALARY),
         sqrt_salary_in_thousands = sqrt(salary_in_thousands))
         
ggplot(just_salary) +
  geom_histogram(mapping = aes(x = SALARY), fill = "yellow", color = "black", bins = 40)+
  ggtitle("Histogram of Salary")

ggplot(just_salary) +
  geom_histogram(mapping = aes(x = log_salary), fill = "yellow", color = "black", bins = 40)+
  ggtitle("Histogram of logSalary")
```

##
Generate relevant data visual plots that explore multicollinearity for the quantitative variables and normality for the quantitative variables as well.  Also, use R code to confirm the levels of the categorical variables.

```{r final_project2}
Quantitative_variables <- Salary_Prediction_of_Data_Professions[, c("AGE", "SALARY", "LEAVES.USED", "LEAVES.REMAINING", "RATINGS", "PAST.EXP")]

#Create a correlation matrix
cor_matrix <- cor(Quantitative_variables, use = "complete.obs")

# Convert the correlation matrix to a data frame
cor_matrix_df <- as.data.frame(as.table(cor_matrix))

cor_matrix_df$Var1 <- recode(cor_matrix_df$Var1,
                             "AGE" = "Age",
                             "SALARY" = "Salary",
                             "LEAVES.USED" = "Leaves Used",
                             "LEAVES.REMAINING" = "Leaves Remaining",
                             "RATINGS" = "Ratings",
                             "PAST.EXP" = "Past Experience")

cor_matrix_df$Var2 <- recode(cor_matrix_df$Var2,
                             "AGE" = "Age",
                             "SALARY" = "Salary",
                             "LEAVES.USED" = "Leaves Used",
                             "LEAVES.REMAINING" = "Leaves Remaining",
                             "RATINGS" = "Ratings",
                             "PAST.EXP" = "Past Experience")

# Plot the heatmap with renamed variables
ggplot(cor_matrix_df, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +  
  scale_fill_gradient2(low = "purple", high = "black", mid = "gold", midpoint = 0) + 
  geom_text(aes(label = round(Freq, 2)), color = "white", size = 4) +  
  labs(title = "Correlation Heatmap of Quantitative Variables") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  
        axis.text.y = element_text(angle = 45, hjust = 1))


# Create a barplot to explore the distribution of "SALARY"
ggplot(Salary_Prediction_of_Data_Professions, aes(x = SALARY)) + 
  geom_histogram(aes(y = ..count../sum(..count..)), bins = 20, fill = "gold", color = "black") + 
  labs(title = "Distribution of Salary", x = "Salary", y = "Proportion") +
  scale_x_continuous(labels = scales::comma)

#Convert SEX and UNIT columns into factors
Salary_Prediction_of_Data_Professions$DESIGNATION <- factor(Salary_Prediction_of_Data_Professions$DESIGNATION)
Salary_Prediction_of_Data_Professions$SEX <- factor(Salary_Prediction_of_Data_Professions$SEX)
Salary_Prediction_of_Data_Professions$UNIT <- factor(Salary_Prediction_of_Data_Professions$UNIT)

#Levels of the categorical variables
levels(Salary_Prediction_of_Data_Professions$DESIGNATION)
levels(Salary_Prediction_of_Data_Professions$SEX)
levels(Salary_Prediction_of_Data_Professions$UNIT)
```
<div class="main-text">
From the correlation matrix we can observe the following regarding multicollinearity: <br>
- **High Correlations**: There is a strong positive correlation between AGE and PAST.EXP (0.90) and between AGE and SALARY (0.87).<br>
- **Low Correlations**: Variables like LEAVES.USED and LEAVES.REMAINING have very weak correlations with other variables suggesting they are not problematic for multicollinearity.<br> <br>
The histogram for SALARY is right-skewed, indicating that most employees earn lower salaries, with fewer employees earning very high salaries. <br> <br>
On this dataset there are 2 categorical variables, SEX and UNIT <br>
- **SEX**: Male, Female <br>
- **UNIT**: Finance, IT, Management, Marketing, Operations, Web <br>
- **DESIGNATION**: Analyst, Associate, Director, Manager, Senior Analyst, Senior Manager
</div>





##
Using R code, produce a full Regression Model that consists of quantitative and categorical variables.  Make use of the R generated dummy variable matrices 

```{r final_project3}
contrasts(Salary_Prediction_of_Data_Professions$SEX)  
contrasts(Salary_Prediction_of_Data_Professions$UNIT)   
contrasts(Salary_Prediction_of_Data_Professions$DESIGNATION)

# Fit the regression model 
model <- lm(SALARY ~ AGE + PAST.EXP + SEX + UNIT + DESIGNATION + RATINGS, data = Salary_Prediction_of_Data_Professions)
model
```
<div class="main-text">
SALARY = 39462.48 + (289.09 × AGE) − (256.60 × PAST.EXP) − (278.91 × SEXM) − (696.35 × UNITIT) − (309.75 × UNITManagement) − (185.85 × UNITMarketing) − (71.72 × UNITOperations) − (528.95 × UNITWeb) + (40992.74 × DESIGNATIONAssociate) + (240269.43 × DESIGNATIONDirector) + (79871.64×DESIGNATIONManager) + (14086.63 × DESIGNATIONSeniorAnalyst) + (132140.19 × DESIGNATIONSeniorManager) − (138.16 × RATINGS)
</div>


# Max's Section


##
Using only the quantitative variables as predictors, produce a model using matrix methods. Also use matrix methods to find the fitted values and the residuals

```{r final_project4}
# Finding a subsection of the data that is solely quantitative 

quantative_model <- Salary_Prediction_of_Data_Professions %>% 
  select(SALARY, AGE, `PAST EXP`, `LEAVES USED`)

# Eliminating cases with na values to prep for matrix

complete_dataset <- quantative_model[complete.cases(quantative_model),]

quant_model <- lm(SALARY ~ AGE + `PAST EXP` + `LEAVES USED`, data = complete_dataset)
quant_model2 <- lm(SALARY ~ AGE, data = complete_dataset)
#summary(quant_model)
#x <- complete_dataset$AGE
#y <- complete_dataset$SALARY

ggplot(complete_dataset)+
  geom_point(aes(AGE, SALARY))
ggplot(complete_dataset)+
  geom_point(aes(`PAST EXP`, SALARY))

# Age has the highest t value so this is the predictor variable we will use with the matrices

# Creating a matrix for AGE (predictor) with a column of "1s"

quantitative_predictor_with_dummy <- complete_dataset %>% 
  mutate(INTERCEPT = 1) %>% 
  select(INTERCEPT, AGE)
  
x_matrix <- as.matrix(quantitative_predictor_with_dummy)
#head(x_matrix)

# Creating a matrix for the response variable

quantitative_response <- complete_dataset %>% 
  select(SALARY)

y_matrix <- as.matrix(quantitative_response)
#head(y_matrix)

#t(x) -> Transpose of x
#%*% -> row column multiplication
#solve(x) -> inverse of x

# Find Slope and Intercept
solve(t(x_matrix)%*%x_matrix)%*%t(x_matrix)%*%y_matrix -> intercept_and_slope
intercept_and_slope

# Find Residual Values
residuals <- y_matrix -  x_matrix %*% intercept_and_slope
head(residuals)

# Find Fitted values
fitted <- x_matrix %*% intercept_and_slope
head(fitted)

plot(fitted, residuals)
abline(h = 0, col = "blue")

quadratic_model <- lm(complete_dataset$SALARY ~ poly(complete_dataset$AGE, 2))
plot(quadratic_model)
```


##
Produce an output summary table to be used to analyze and evaluate the full model (Adjusted R squared, Standard Error, Significance of Variables, ect…)

```{r final_project5}
full_model_prep <- Salary_Prediction_of_Data_Professions %>% 
  select(SALARY, AGE, SEX, DESIGNATION, UNIT, `PAST EXP`, `LEAVES USED`, RATINGS)

full_model <- lm(SALARY ~ AGE + SEX + DESIGNATION + UNIT + `PAST EXP` + `LEAVES USED` + RATINGS, data = full_model_prep)
summary(full_model)
```
<div class="main-text">Age (p < 0.05) and Job Designation (p < 0.00001) are both statistically significant, indicating a very high probability that the change seen in the response (salary) is not due to chance alone.</div>

##
Use procedures and techniques explored in class to produce confidence intervals for the independent quantitative variables of your model. Choose at least two of the quantitative variables to find confidence intervals for.

```{r final_project6}

confidence_interval_model <- Salary_Prediction_of_Data_Professions %>% 
  select(SALARY, AGE, `PAST EXP`)

# Create models for two different quantitative models

conf_model_age <- lm(SALARY ~ AGE, data = confidence_interval_model)
conf_model_exp <- lm(SALARY ~ `PAST EXP`, data = confidence_interval_model)

# Find b and se

summary(conf_model_age)
## b = 8232.9, se = 89.96
summary(conf_model_exp)
## b = 11543.2, se = 136.9

# Calculate t scores
t_score <- qt(p = 0.025, df = 2632 , lower.tail = FALSE)
#t_score 1.96

# calculate Age confidence interval
age_UB <- 8232.9 + t_score*89.96
age_LB <- 8232.9 - t_score*89.96

# calculate Past Exp confidence intervals
past_exp_UB <- 11543.2 + t_score*136.9
past_exp_LB <- 11543.2 - t_score*136.9


age_UB
age_LB
past_exp_UB
past_exp_LB
```

# Conie's Section


##
Now produce a reduced model (removing variables of your choice with justification). Use R summary coding for both models and offer justification for choosing one model over the other.

```{r final_project7}
# review summary statistics of full model
summary(full_model)

# build reduced model
reduced_model <- lm(SALARY ~ DESIGNATION + AGE, data = full_model_prep)

#review summary statistics of reduced model
summary(reduced_model)

# confirm with stepwise regression
sw_intercept <- lm(SALARY ~ 1, data = full_model_prep)
sw_intercept

sw_all <- full_model

sw_forward <- step(sw_intercept, direction = 'forward', scope = formula(sw_all), trace = 0)
sw_forward$anova

#final reduced model
final_reduced_model <- lm(SALARY ~ DESIGNATION, data = full_model_prep)
summary(final_reduced_model)
final_reduced_model
```

<div class="main-text">After reviewing the summary statistics of the full model, we can remove the variables Unit, Past Exp, Leaves Used, and Sex because none are statistically significant. For all variables, p > 0.05. We can reduce the model to include only Age and Designation, who are significant because p < 0.05 in the full model. After reducing the model, we see that the only significant variable is Designation - Age now has p > 0.05. After running a stepwise regression to confirm the outputs, our final reduced model only includes the indicator variable Designation.</div>


##
Research and apply a model analysis technique not discussed in class to your full model or reduced model.  Fully explain the technique or procedure and how it is being applied to your specific model.

```{r final_project8}
# Remove rows with missing values
data_cleaned <- na.omit(full_model_prep[c("AGE", "SALARY", "DESIGNATION", "LEAVES USED", "RATINGS", "PAST EXP",
                                          "RATINGS", "UNIT")])

data_cleaned$DESIGNATION <- as.factor(data_cleaned$DESIGNATION)
data_cleaned$UNIT <- as.factor(data_cleaned$UNIT)

# Fit a polynomial regression model (degree = 2)
polymodel <- lm(SALARY ~ poly(AGE, 2) + DESIGNATION +  poly(UNIT, 2) + poly(`PAST EXP`, 2) +
                  poly(`LEAVES USED`, 2) + poly(RATINGS, 2), data = data_cleaned)
summary(polymodel)

# perform a stepwise regression with the polynomial model
sw_poly_intercept <- lm(SALARY ~ 1, data = data_cleaned)
sw_poly_forward <- step(sw_poly_intercept, direction = 'forward', scope = formula(polymodel), trace = 0)
sw_poly_forward$anova

# build a final reduced model and review summary statistics
final_polymodel <- lm(SALARY ~ DESIGNATION + poly(AGE, 2) + poly (`PAST EXP`, 2), data = data_cleaned)
final_polymodel
summary(final_polymodel)
```

<div class="header-text"><center>Polynomial Model</center></div><br><br>
<div class="main-text">I applied a polynomial model and combined it with a stepwise regression to address the nonlinear relationships of other variables (Age, Past Exp, etc.). <br><br>
1. Data Prep - I had to clean the data a little further by removing any NA values and then factoring the categorical variables (Designation and Unit) before building a polynomial model. <br><br>
2. Model Adjustment - Since Designation was statistically significant as a linear variable, I did not adjust it to a polynomial. All other variables I changed to polynomials and created a model. <br><br>
3. Model Assessment & Tuning - After creating the model and running summary statistics, Age and Past Exp were now statistically significant (p < 0.05) and the model explained the variation in Salary well (Adjusted R-squared = 0.9546). However, there were a number of insignificant terms (p > 0.05). I performed a stepwise regression to confirm the best predictor variables, which were Designation, Age^2, and Past Exp^2. I then built a reduced model and computed the summary statistics to confirm significance of all variables - all variables were significant (p < 0.05) and the model had an Adjusted R-squared value of 0.9543.<br><br>
4. Final Model <br><br> <center>y = B0 + (B1 * DESIGNATION) + (B2 * AGE) + (B3 * AGE)^2 + (B4 * PAST EXP) + (B5 * PAST EXP)^2 + e </center></div>

##
Offer final summary perspectives about the data and the models that you produce, suggesting how your models or model analysis enhanced your understanding of the data. (4 or 5 sentences)

<div class="header-text"><center>Our Changing Perspectives</center></div>
<div class="main-text">Our first full model gives us a wide view of the data and includes a number of variables that we would conventionally associate with salary, like Age, Sex, Past Experience, Designation. After analyzing the summary statistics of the data, we can see that most of these variables are not statistically significant to predicting the salaries of data science professionals from this data set. Even after keeping the statistically significant variables, our reduced model proved to eliminate further variables, leaving only Designation as a statistically significant predictor of salary. Using a polynomial model allowed us to better fit a model to the data that we had available, accounting for some of the nonlinear trends. This is more of what we expected to see from the beginning of the project, but something that may not be easily captured in a strictly linear relationship.</div>
<br><br>
<div class="header-text"><center>What does this mean?</center></div>
<div class="main-text">This means that our data set is potentially too narrow. Even though we have over 2600 entries, this may not be a large enough sample size to overcome the effects of the designation variable. In a deeper investigation, we may need to gather data from other sources to see if the Designation (which we may term as any sort of job level variable) is a powerful indicator across the board in this industry, or if this is a result of our data selection.</div>
