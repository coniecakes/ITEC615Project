---
title: "Final Project STAT-615"
author: "Maxwell Miller-Golub"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 1000)
library(tidyverse)
library(readr)
```

```{r data}
# Set current working directory to same folder that houses data file if you haven't done so already.

local_filename <- "Salary Prediction of Data Professions.csv"

Salary_Prediction_of_Data_Professions <- read_csv(local_filename)

view(Salary_Prediction_of_Data_Professions)
```

# Petro's Section

### 1)	Offer a preliminary description of the data set. For example, indicate the size of the data source, describe the variables, and include any other data profile information that would be of interest.

```{r final_project1}
```

### 2)	Generate relevant data visual plots that explore multicollinearity for the quantitative variables and normality for the quantitative variables as well.  Also, use R code to confirm the levels of the categorical variables.

```{r final_project2}
```


### 3)	Using R code, produce a full Regression Model that consists of quantitative and categorical variables.  Make use of the R generated dummy variable matrices 

```{r final_project3}
```



# Max's Section


### 4)	Using only the quantitative variables as predictors, produce a model using matrix methods. Also use matrix methods to find the fitted values and the residuals

```{r final_project4}
# Finding a subsection of the data that is solely quantitative 

quantative_model <- Salary_Prediction_of_Data_Professions %>% 
  select(SALARY, AGE, `PAST EXP`, `LEAVES USED`)

# Eliminating cases with na values to prep for matrix

complete_dataset <- quantative_model[complete.cases(quantative_model),]

quant_model <- lm(SALARY ~ AGE + `PAST EXP` + `LEAVES USED`, data = complete_dataset)
quant_model2 <- lm(SALARY ~ AGE, data = complete_dataset)
#summary(quant_model)
#x <- complete_dataset$AGE
#y <- complete_dataset$SALARY

ggplot(complete_dataset)+
  geom_point(aes(AGE, SALARY))
ggplot(complete_dataset)+
  geom_point(aes(`PAST EXP`, SALARY))

# Age has the highest t value so this is the predictor variable we will use with the matrices

# Creating a matrix for AGE (predictor) with a column of "1s"

quantitative_predictor_with_dummy <- complete_dataset %>% 
  mutate(INTERCEPT = 1) %>% 
  select(INTERCEPT, AGE)
  
x_matrix <- as.matrix(quantitative_predictor_with_dummy)
#head(x_matrix)

# Creating a matrix for the response variable

quantitative_response <- complete_dataset %>% 
  select(SALARY)

y_matrix <- as.matrix(quantitative_response)
#head(y_matrix)

#t(x) -> Transpose of x
#%*% -> row column multiplication
#solve(x) -> inverse of x

# Find Slope and Intercept
solve(t(x_matrix)%*%x_matrix)%*%t(x_matrix)%*%y_matrix -> intercept_and_slope
intercept_and_slope

# Find Residual Values
residuals <- y_matrix -  x_matrix %*% intercept_and_slope
head(residuals)

# Find Fitted values
fitted <- x_matrix %*% intercept_and_slope
head(fitted)

plot(fitted, residuals)
abline(h = 0, col = "blue")
```


### 5)	Produce an output summary table to be used to analyze and evaluate the full model (Adjusted R squared, Standard Error, Significance of Variables, ectâ€¦)

```{r final_project5}
full_model_prep <- Salary_Prediction_of_Data_Professions %>% 
  select(SALARY, AGE, SEX, DESIGNATION, UNIT, `PAST EXP`, `LEAVES USED`, RATINGS)

full_model <- lm(SALARY ~ AGE + SEX + DESIGNATION + UNIT + `PAST EXP` + `LEAVES USED` + RATINGS, data = full_model_prep)
summary(full_model)
```
Age (p < 0.05) and Job Designation (p < 0.00001) are both statistically significant, indicating a very high probability that the change seen in the response (salary) is not due to chance alone.

### 6)	Use procedures and techniques explored in class to produce confidence intervals for the independent quantitative variables of your model. Choose at least two of the quantitative variables to find confidence intervals for.

```{r final_project6}
# calculation t scores
qt(p = 0.025, df = 2632 , lower.tail = FALSE) -> t_score

# calculate Past Exp confidence intervals
past_exp_UB <- -260.78 + t_score*154.69
past_exp_LB <- -260.78 - t_score*154.69

# calculate Age confidence interval
age_UB <- 285.76 + t_score*144.59
age_LB <- 285.76 - t_score*144.59
```

# Conie's Section


### 7)	Now produce a reduced model (removing variables of your choice with justification). Use R summary coding for both models and offer justification for choosing one model over the other.

```{r final_project7}
# review summary statistics of full model
summary(full_model)

# build reduced model
reduced_model <- lm(SALARY ~ DESIGNATION + AGE, data = full_model_prep)

#review summary statistics of reduced model
summary(reduced_model)

# confirm with stepwise regression
sw_intercept <- lm(SALARY ~ 1, data = full_model_prep)
sw_intercept

sw_all <- full_model

sw_forward <- step(sw_intercept, direction = 'forward', scope = formula(sw_all), trace = 0)
sw_forward$anova
```
After reviewing the summary statistics of the full model, we can remove the variables Unit, Past Exp, Leaves Used, and Sex because none are statistically significant. For all variables, p > 0.05. We can reduce the model to include only Age and Designation, who are significant because p < 0.05 in the full model. After reducing the model, we see that the only significant variable is Designation - Age now has p > 0.05. After running a stepwise regression to confirm the outputs, our final reduced model only includes the indicator variable Designation.

### 8)	Research and apply a model analysis technique not discussed in class to your full model or reduced model.  Fully explain the technique or procedure and how it is being applied to your specific model.

```{r final_project8}

```


### 9)	Offer final summary perspectives about the data and the models that you produce, suggesting how your models or model analysis enhanced your understanding of the data.   (4 or 5 sentences)

```{r final_project9}
```

